# Awesome-vision-language-action-models [![Awesome](https://awesome.re/badge.svg)](https://awesome.re) 
üî• **Latest Advances on Vison-Language-Action Models. ** (*UNDERCONSTRUCTION* üî•)

Embodied intelligence is one of the most critical carrier for general artificial intelligence to reach the physical world, with significant implications for our daily lives. Recent years have seen tremendous advancements in robotics technology, and the AI community is increasingly turning its attention to robots. We're excitedly anticipating the emergence of a GPT-like breakthrough in robotics! ü§ó
<!-- ![](resources/vla.gif) -->

## Trending Projects

- [ü§óLeRoBot](https://github.com/huggingface/lerobot) - Making AI for Robotics more accessible with end-to-end learning.

## Table of Content

- [Awesome-vision-language-action-models ](#awesome-vision-language-action-models-)
  - [Trending Projects](#trending-projects)
  - [Table of Content](#table-of-content)
  - [Milestone Papers](#milestone-papers)
  - [Datasets \& Benchmark](#datasets--benchmark)
  - [Tutorials and Courses](#tutorials-and-courses)
  - [Contributing](#contributing)

## Milestone Papers

|  Date   |       keywords        |       Institute       | Paper                                                                                                                              |                                              Code                                               |
| :-----: | :-------------------: | :-------------------: | ---------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------: |
| 2022-12 |      Transformer      |        Google         | [RT-1: Robotics Transformer for real-world control at scale](https://arxiv.org/abs/2212.06817)                                     | [google-research/robotics_transformer](https://github.com/google-research/robotics_transformer) |
| 2023-03 |  General Multimodal   |        Google         | [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378)                                                  |                                                -                                                |
| 2023-07 |           -           |        Google         | [RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control](https://arxiv.org/abs/2307.15818)                  |                                                -                                                |
| 2024-06 |      Open-Source      |                       | [OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/abs/2406.09246)                                           |                      [openvla/openvla](https://github.com/openvla/openvla)                      |
| 2024-05 |    Low-Cost/Design    |        Google         | [ALOHA 2: An Enhanced Low-Cost Hardware for Bimanual Teleoperation](https://arxiv.org/abs/2405.02292)                              |            [tonyzhaozh/aloha](https://github.com/tonyzhaozh/aloha/tree/main/aloha2)             |
| 2024-09 |     Heterogeneous     |          MIT          | [Heterogenous Pre-trained Transformers](https://arxiv.org/pdf/2409.20537)                                                          |                           [liruiw/HPT](https://github.com/liruiw/HPT)                           |
| 2023-04 |       Diffusion       |  Columbia University  | [Diffusion Policy: Visuomotor Policy Learning via Action Diffusion](https://www.roboticsproceedings.org/rss19/p026.pdf)            |       [real-stanford/diffusion_policy](https://github.com/real-stanford/diffusion_policy)       |
| 2024-10 |  Bimanual Diffusion   |          THU          | [RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation](https://arxiv.org/pdf/2410.07864)                                 |  [thu-ml/RoboticsDiffusionTransformer](https://github.com/thu-ml/RoboticsDiffusionTransformer)  |
| 2024-05 |       Diffusion       |      UC Berkeley      | [Octo: An Open-Source Generalist Robot Policy](https://arxiv.org/pdf/2405.12213)                                                   |                     [octo-models/octo](https://github.com/octo-models/octo)                     |
| 2024-10 |     Flow Matching     | Physical Intelligence | [œÄ0: A Vision-Language-Action Flow Model for General Robot Control](https://arxiv.org/pdf/2410.24164)                              |                                                -                                                |
| 2024-03 |  Vector Quantization  |                       | [Behavior Generation with Latent Actions](https://arxiv.org/pdf/2403.03181)                                                        |           [jayLEE0301/vq_bet_official](https://github.com/jayLEE0301/vq_bet_official)           |
| 2024-10 | Video-Language-Action |       ByteDance       | [GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation](https://arxiv.org/abs/2410.06158) |                                                -                                                |
| 2023-10 |  Action-in-Video-Out  |      UC Berkeley      | [UniSim: Learning Interactive Real-World Simulators](https://arxiv.org/pdf/2310.06114)                                             |                                                -                                                |
| 2023-04 |    Action Chunking    |                       | [Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware (ACT](https://arxiv.org/pdf/2304.13705)                        |                       [tonyzhaozh/act](https://github.com/tonyzhaozh/act)                       |


## Datasets & Benchmark

|  Date   |   keywords    | Institute | Paper                                                                                                   |                                           Code                                            |
| :-----: | :-----------: | :-------: | ------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------: |
| 2023-10 | OpenX Dataset |     -     | [Open X-Embodiment: Robotic Learning Datasets and RT-X Models](https://arxiv.org/pdf/2310.08864)        | [google-deepmind/open_x_embodiment](https://github.com/google-deepmind/open_x_embodiment) |
| 2024-08 | Data Mixtures | Stanford  | [Re-Mix: Optimizing Data Mixtures for Large Scale Imitation Learning](https://arxiv.org/pdf/2408.14037) |                      [jhejna/remix](https://github.com/jhejna/remix)                      |


## Tutorials and Courses

|  Date   |                                                                                       keywords                                                                                       |
| :-----: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| 2024-11 | [CoRL24-8 From Octo to œÄ‚ÇÄ: How to Train Your Generalist Robot Policy](https://www.bilibili.com/video/BV1p3UYYxEFb/?share_source=copy_web&vd_source=7b9c04cb5a01c024b1b34f587bb769ce) |
| 2024-10 |                             [RDT-1B Talk](https://www.bilibili.com/video/BV1FjyHYmEDQ/?share_source=copy_web&vd_source=7b9c04cb5a01c024b1b34f587bb769ce)                             |
| 2024-08 |                                       [OpenVLA: LeRobot Research Presentation #5 by Moo Jin Kim](https://www.youtube.com/watch?v=-0s0v3q7mBk)                                        |


## Contributing

This is an active repository and your contributions are always welcome!

The repository is underconstruction and I will keep some pull requests open if I'm not sure if they are awesome for VLA Model, you could vote for them by adding üëç to them.

---

If you have any question about this opinionated list, do not hesitate to contact me üìÆ dlqu22@m.fudan.edu.cn.